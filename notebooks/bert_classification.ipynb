{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import random \n",
    "import spacy\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.10\n",
    "train_valid_ratio = 0.80\n",
    "first_n_words = 512\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\", disable = ['ner', 'tagger', 'parser', 'textcat'])\n",
    "#nlp.max_length = 10000000\n",
    "\n",
    "df_ml = pd.read_csv(\"temp_vn.csv\", encoding=\"latin1\", parse_dates=True)\n",
    "df_ml = df_ml[df_ml.len != -1]\n",
    "df_ml = df_ml[~df_ml.id.isna()]\n",
    "df_ml.id = df_ml.id.astype(int)\n",
    "#df_ml = df_ml.sample(n=N_SAMPLE)  # random subsample to get going quicker\n",
    "df_ml['text'] = \"zzz\"\n",
    "df_ml = df_ml.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>version_number</th>\n",
       "      <th>bill_id</th>\n",
       "      <th>signed</th>\n",
       "      <th>partisan_lean</th>\n",
       "      <th>len</th>\n",
       "      <th>sc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2119301</td>\n",
       "      <td>1</td>\n",
       "      <td>1092981</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236118</td>\n",
       "      <td>8102</td>\n",
       "      <td>562-1</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2708948</td>\n",
       "      <td>2</td>\n",
       "      <td>1092981</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236118</td>\n",
       "      <td>9545</td>\n",
       "      <td>562-1</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2708949</td>\n",
       "      <td>3</td>\n",
       "      <td>1092981</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236118</td>\n",
       "      <td>10714</td>\n",
       "      <td>562-1</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2708950</td>\n",
       "      <td>4</td>\n",
       "      <td>1092981</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236118</td>\n",
       "      <td>1239</td>\n",
       "      <td>562-1</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2708951</td>\n",
       "      <td>5</td>\n",
       "      <td>1092981</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236118</td>\n",
       "      <td>133</td>\n",
       "      <td>562-1</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496245</th>\n",
       "      <td>2740636</td>\n",
       "      <td>1</td>\n",
       "      <td>1346334</td>\n",
       "      <td>0</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>163</td>\n",
       "      <td>666-2</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496246</th>\n",
       "      <td>2740995</td>\n",
       "      <td>1</td>\n",
       "      <td>1346470</td>\n",
       "      <td>0</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>879</td>\n",
       "      <td>666-2</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496247</th>\n",
       "      <td>2741134</td>\n",
       "      <td>1</td>\n",
       "      <td>1346569</td>\n",
       "      <td>0</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>3603</td>\n",
       "      <td>666-2</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496248</th>\n",
       "      <td>2741135</td>\n",
       "      <td>1</td>\n",
       "      <td>1346570</td>\n",
       "      <td>0</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>345</td>\n",
       "      <td>666-2</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496249</th>\n",
       "      <td>2742709</td>\n",
       "      <td>1</td>\n",
       "      <td>1347123</td>\n",
       "      <td>0</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>478</td>\n",
       "      <td>666-2</td>\n",
       "      <td>zzz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496250 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  version_number  bill_id  signed  partisan_lean    len  sc_id  \\\n",
       "0       2119301               1  1092981       0       0.236118   8102  562-1   \n",
       "1       2708948               2  1092981       0       0.236118   9545  562-1   \n",
       "2       2708949               3  1092981       0       0.236118  10714  562-1   \n",
       "3       2708950               4  1092981       0       0.236118   1239  562-1   \n",
       "4       2708951               5  1092981       0       0.236118    133  562-1   \n",
       "...         ...             ...      ...     ...            ...    ...    ...   \n",
       "496245  2740636               1  1346334       0       0.525000    163  666-2   \n",
       "496246  2740995               1  1346470       0       0.525000    879  666-2   \n",
       "496247  2741134               1  1346569       0       0.525000   3603  666-2   \n",
       "496248  2741135               1  1346570       0       0.525000    345  666-2   \n",
       "496249  2742709               1  1347123       0       0.525000    478  666-2   \n",
       "\n",
       "       text  \n",
       "0       zzz  \n",
       "1       zzz  \n",
       "2       zzz  \n",
       "3       zzz  \n",
       "4       zzz  \n",
       "...     ...  \n",
       "496245  zzz  \n",
       "496246  zzz  \n",
       "496247  zzz  \n",
       "496248  zzz  \n",
       "496249  zzz  \n",
       "\n",
       "[496250 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:43:23.727699\n"
     ]
    }
   ],
   "source": [
    "def trim_string(x):\n",
    "    fname = '../data/raw/' + str(df_ml.iloc[i]['id']) +'.txt'\n",
    "    x = open(fname).read()\n",
    "    x = x.split(maxsplit=first_n_words)\n",
    "    x = ' '.join(x[:first_n_words])\n",
    "\n",
    "    return x\n",
    "\n",
    "#def clipped_text(i):\n",
    "\n",
    "    \n",
    "#    tokens = nlp(d1)\n",
    "#    str_tokens = [token.orth_ for token in tokens[:512]]\n",
    "#    df_ml.at[i, 'text'] = \" \".join(str_tokens)\n",
    "#    return(1)\n",
    "    \n",
    "startTime2 = datetime.now()\n",
    "for i in range(len(df_ml)):\n",
    "    df_ml.at[i, 'text'] = trim_string(i)\n",
    "print(datetime.now()-startTime2)\n",
    "\n",
    "# 43 min to process all the datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 496250 entries, 0 to 496249\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   id              496250 non-null  int64  \n",
      " 1   version_number  496250 non-null  int64  \n",
      " 2   bill_id         496250 non-null  int64  \n",
      " 3   signed          496250 non-null  int64  \n",
      " 4   partisan_lean   496250 non-null  float64\n",
      " 5   len             496250 non-null  int64  \n",
      " 6   sc_id           496250 non-null  object \n",
      " 7   text            496250 non-null  object \n",
      "dtypes: float64(1), int64(5), object(2)\n",
      "memory usage: 30.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_ml.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data_path = '~/Downloads/news.csv'\n",
    "\n",
    "#destination_folder = 'bert_cl/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signed</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HOUSE BILL 1 By Staples SENATE BILL 16 By Dick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Senate Government Operations Committee 1 Amend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Senate State and Local Government Committee 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Senate Finance, Ways, and Means Committee 1 Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Amendment No. 4 to SB0016 Dickerson Signature ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signed                                               text\n",
       "0       0  HOUSE BILL 1 By Staples SENATE BILL 16 By Dick...\n",
       "1       0  Senate Government Operations Committee 1 Amend...\n",
       "2       0  Senate State and Local Government Committee 1 ...\n",
       "3       0  Senate Finance, Ways, and Means Committee 1 Am...\n",
       "4       0  Amendment No. 4 to SB0016 Dickerson Signature ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert = df_ml[['signed', 'text']]\n",
    "df_bert.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = 'bert_classification'\n",
    "\n",
    "# Train-test split\n",
    "df_train_full, df_test = train_test_split(df_bert, train_size = train_test_ratio, random_state = 1)\n",
    "\n",
    "# Train-valid split\n",
    "df_train, df_valid = train_test_split(df_train_full, train_size = train_valid_ratio, random_state = 1)\n",
    "\n",
    "# Write preprocessed data\n",
    "df_train.to_csv(destination_folder + '/train.csv', index=False)\n",
    "df_valid.to_csv(destination_folder + '/valid.csv', index=False)\n",
    "df_test.to_csv(destination_folder + '/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/repos/govhawk_ml/env/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/luke/repos/govhawk_ml/env/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/luke/repos/govhawk_ml/env/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (654 > 512). Running this sequence through the model will result in indexing errors\n",
      "/home/luke/repos/govhawk_ml/env/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/luke/repos/govhawk_ml/env/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and prepare\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Model parameter\n",
    "MAX_SEQ_LEN = first_n_words\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 4\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('label', label_field), ('text', text_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=destination_folder, train='train.csv', validation='valid.csv',\n",
    "                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "# Iterators\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.Iterator at 0x7f7d547fdbb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/luke/repos/govhawk_ml/env/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [4962/198500], Train Loss: 0.5403, Valid Loss: 0.5612\n",
      "Model saved to ==> bert_classification/model.pt\n",
      "Model saved to ==> bert_classification/metrics.pt\n",
      "Epoch [1/20], Step [9924/198500], Train Loss: 0.5772, Valid Loss: 0.5893\n",
      "Epoch [2/20], Step [14886/198500], Train Loss: 0.5503, Valid Loss: 0.5906\n",
      "Epoch [2/20], Step [19848/198500], Train Loss: 0.5915, Valid Loss: 0.5859\n",
      "Epoch [3/20], Step [24810/198500], Train Loss: 0.5497, Valid Loss: 0.5886\n",
      "Epoch [3/20], Step [29772/198500], Train Loss: 0.5922, Valid Loss: 0.6022\n",
      "Epoch [4/20], Step [34734/198500], Train Loss: 0.5486, Valid Loss: 0.5865\n",
      "Epoch [4/20], Step [39696/198500], Train Loss: 0.5890, Valid Loss: 0.5836\n",
      "Epoch [5/20], Step [44658/198500], Train Loss: 0.5520, Valid Loss: 0.5867\n",
      "Epoch [5/20], Step [49620/198500], Train Loss: 0.5890, Valid Loss: 0.5836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4b83b2ad6254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-4b83b2ad6254>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, valid_loader, num_epochs, eval_every, file_path, best_valid_loss)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/govhawk_ml/env/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/govhawk_ml/env/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = NUM_EPOCHS,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for (labels, text), _ in train_loader:\n",
    "            labels = labels.type(torch.LongTensor)           \n",
    "            labels = labels.to(device)\n",
    "            text = text.type(torch.LongTensor)  \n",
    "            text = text.to(device)\n",
    "            output = model(text, labels)\n",
    "            loss, _ = output\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                    # validation loop\n",
    "                    for (labels, text), _ in valid_loader:\n",
    "                        labels = labels.type(torch.LongTensor)           \n",
    "                        labels = labels.to(device)\n",
    "                        text = text.type(torch.LongTensor)  \n",
    "                        text = text.to(device)\n",
    "                        output = model(text, labels)\n",
    "                        loss, _ = output\n",
    "                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')\n",
    "\n",
    "model = BERT().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (labels, text), _ in test_loader:\n",
    "\n",
    "                labels = labels.type(torch.LongTensor)           \n",
    "                labels = labels.to(device)\n",
    "                text = text.type(torch.LongTensor)  \n",
    "                text = text.to(device)\n",
    "                output = model(text, labels)\n",
    "\n",
    "                _, output = output\n",
    "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "                y_true.extend(labels.tolist())\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['Signed', 'Failed'])\n",
    "    ax.yaxis.set_ticklabels(['Signed', 'Failed'])\n",
    "    \n",
    "best_model = BERT().to(device)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "\n",
    "evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5966+723)/(5966+723+638+1673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 samples, 5 epochs, 128 seq_len:  accuracy = 0.743\n",
    "# 100,000 samples, 5 epochs, 128 seq_len: accuracy = 0.818\n",
    "# 10,000 samples, 5 epochs, 512 seq_len, accuracy = 0.747\n",
    "# 10,000 samples, 5 epochs, 512 seq len, accuracy = 0.687\n",
    "print(datetime.now() - startTime)\n",
    "print(\"sequence length:{}, N samples:{}, time:{}\".format(MAX_SEQ_LEN, N_SAMPLE, datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "govhawk_kernel",
   "language": "python",
   "name": "govhawk_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
