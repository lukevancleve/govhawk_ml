{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.data.read_parallel import read_parallel_local\n",
    "from src.models.deeplegis import *\n",
    "from src.models.data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of examples: 199646\n",
      "Reduced number of examples:  199646\n",
      "Took 8.991032763322194 min to open 199646 files with 20 processes.\n"
     ]
    }
   ],
   "source": [
    "REDUCE_BY_FACTOR = 1 # Make the dataset smaller for development purposes\n",
    "train_test_ratio = 0.91\n",
    "train_valid_ratio = 0.90\n",
    "\n",
    "if 'DATA_VOL' not in os.environ:\n",
    "    # Manually set:\n",
    "    DATA_VOL = '/home/luke/tmp_vol/'\n",
    "else:\n",
    "    DATA_VOL = os.environ['DATA_VOL']\n",
    "    \n",
    "# Pre-wrangled metadata\n",
    "df = pd.read_csv(\"../references/derived/ml_data.csv\", encoding=\"latin1\", parse_dates=True)\n",
    "df.id = df.id.astype(int)\n",
    "sc_id_encoder = LabelEncoder()\n",
    "df['sc_id_cat'] = sc_id_encoder.fit_transform(df['sc_id'])\n",
    "\n",
    "print(f\"Original number of examples: {len(df)}\")\n",
    "if REDUCE_BY_FACTOR != 1:\n",
    "    df = df.sample(n=int(len(df)/REDUCE_BY_FACTOR)) #\n",
    "print(f\"Reduced number of examples:  {len(df)}\")\n",
    "\n",
    "df['text'] = read_parallel_local(df['id'], DATA_VOL + \"/clean/\")\n",
    "df = df[~df.text.isna()]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.text.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: (161092, 8)\n",
      "Validation size: (17900, 8)\n",
      "Test size: (17703, 8)\n",
      "Building from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distilbert (TFDistilBertMainLay TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           distilbert[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 768)          0           tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "version_number (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sc_id (InputLayer)              [(None, 134)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 903)          0           dropout_20[0][0]                 \n",
      "                                                                 version_number[0][0]             \n",
      "                                                                 sc_id[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 700)          632800      tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 700)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            701         dropout_21[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 66,996,381\n",
      "Trainable params: 66,996,381\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:592: UserWarning: Input dict contained keys ['partisan_lean'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5034/5034 [==============================] - 24313s 5s/step - loss: 0.2619 - binary_accuracy: 0.8960 - precision: 0.6947 - recall: 0.4146 - auc: 0.8776 - val_loss: 0.2084 - val_binary_accuracy: 0.9164 - val_precision: 0.7723 - val_recall: 0.5601 - val_auc: 0.9321\n",
      "\n",
      "Epoch 00001: saving model to /home/luke/tmp_vol/models/distil_bert/distil_bert.ckpt\n",
      "Epoch 2/10\n",
      "5034/5034 [==============================] - 23525s 5s/step - loss: 0.2001 - binary_accuracy: 0.9204 - precision: 0.7679 - recall: 0.6029 - auc: 0.9360 - val_loss: 0.2100 - val_binary_accuracy: 0.9192 - val_precision: 0.8052 - val_recall: 0.5471 - val_auc: 0.9363\n",
      "\n",
      "Epoch 00002: saving model to /home/luke/tmp_vol/models/distil_bert/distil_bert.ckpt\n",
      "Epoch 3/10\n",
      "5034/5034 [==============================] - 23324s 5s/step - loss: 0.1909 - binary_accuracy: 0.9247 - precision: 0.7838 - recall: 0.6236 - auc: 0.9416 - val_loss: 0.2194 - val_binary_accuracy: 0.9147 - val_precision: 0.7350 - val_recall: 0.5981 - val_auc: 0.9266\n",
      "\n",
      "Epoch 00003: saving model to /home/luke/tmp_vol/models/distil_bert/distil_bert.ckpt\n",
      "Epoch 4/10\n",
      " 726/5034 [===>..........................] - ETA: 5:17:25 - loss: 0.1862 - binary_accuracy: 0.9292 - precision: 0.7876 - recall: 0.6770 - auc: 0.9460"
     ]
    }
   ],
   "source": [
    "from src.models.deeplegis import *\n",
    "config = {}\n",
    "config['build_from_scratch'] = True\n",
    "config['model_name'] = 'distil_bert'\n",
    "config['max_length'] = 128\n",
    "config['train_batch_size'] = 32\n",
    "config['testing'] = False\n",
    "config['train_test_ratio'] = 0.91\n",
    "config['train_valid_ratio'] = 0.90 \n",
    "config['tokenizer'] = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "config['n_sc_id_classes'] = len(sc_id_encoder.classes_)\n",
    "config['checkpoint_path'] = DATA_VOL + \"models/\" + config['model_name'] +\"/\" + config['model_name'] +\".ckpt\"\n",
    "config['log_dir'] = DATA_VOL + \"logs/fit/\"+config['model_name']+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "config['epochs'] = 10\n",
    "config['learning_rate'] = 1e-4\n",
    "config['no_text_dense_layer_initialization_path']  = DATA_VOL + \"models/\"+config['model_name']+\"/full_modol.h5\"\n",
    "config['model_location'] = model_location = DATA_VOL + \"models/\"+config['model_name']+\"/full_model.h5\"\n",
    "\n",
    "dl_all = deepLegisDistillBertAll(config)\n",
    "dl_all.load_data(df)\n",
    "dl_all.build()\n",
    "dl_all.deep_legis_model.summary()\n",
    "dl_all.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dl_all.deep_legis_model.save(model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model_location'] = model_location = DATA_VOL + \"models/\"+config['model_name']+\"/full_model.h5\"\n",
    "\n",
    "dl_all2 = deepLegisDistillBertAll(config)\n",
    "#dl_all.load_data(df)\n",
    "dl_all2.build()\n",
    "#dl_all.deep_legis_model.summary()\n",
    "#dl_all.train("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
