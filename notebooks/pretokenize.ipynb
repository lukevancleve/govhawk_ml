{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extensive-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.data.read_parallel import read_parallel_local\n",
    "from src.models.deeplegis import *\n",
    "from src.models.data_loader import *\n",
    "from src.models.configurationClasses import deepLegisConfig\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "import pandarallel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-hunter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of examples: 199646\n",
      "Loading 199646 text files\n",
      "Took 3.4365577975908916 min (206.1934735774994 sec)to open 199646 files with 20 processes.\n",
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "config = deepLegisConfig(\"bert_128.json\", project_root=\"../\")\n",
    "df, encoder = createDeepLegisDataFrame(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compound-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"/home/luke/ml_govhawk_prod_output/preprocessed_df_128.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "combined-malta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (128,), types: tf.int32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.Dataset.from_tensor_slices((df.tokens.to_list()))\n",
    "#tf.data.Dataset.from_tensor_slices([(1,3), (2,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "spatial-maryland",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'as_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-24358b433f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'as_list' is not defined"
     ]
    }
   ],
   "source": [
    "as_list(df.tokens.values[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-luxury",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-polish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "amended-purse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "Tokenized in 4.5029 seconds\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "# Set CPU as available physical device\n",
    "my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "#tokenizer(df.text[0:1].to_list(), return_tensors=\"tf\",  truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "#tokens = tokenizer(df.text[0:2].to_list(), truncation=True, padding=True, max_length=128)\n",
    "#print(len(tokens['input_ids'][0]))\n",
    "#print(len(tokens['input_ids'][1]))\n",
    "\n",
    "def tokenize(text):\n",
    "    ids = tokenizer(text, truncation=True, padding=True, max_length=128)\n",
    "    return tf.cast(ids['input_ids'], 'int32')\n",
    "import time\n",
    "\n",
    "tic = time.perf_counter()\n",
    "df['tokenized'] = df.text.parallel_apply(tokenize)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Tokenized in {toc - tic:0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "irish-virginia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
       "array([  101,  5900,  6372,  3180,  5219,  2000,  1024, 14814,  1037,\n",
       "        2011,  1024,  4387,  6243,  2160,  3021,  2019,  2552,  2000,\n",
       "       27950,  2930,  5900,  3642,  1997,  2000,  3166,  4697,  1996,\n",
       "        5900,  4879,  1997, 27290,  2000, 27764,  1997,  4895, 25154,\n",
       "        2098,  2512, 29278, 21156,  2098,  3200,  1999,  1996,  2168,\n",
       "        5450,  2004, 15726,  3200,  1025,  1998,  2005,  3141,  5682,\n",
       "        1012,  2022,  2009, 11955,  2011,  1996,  6372,  1997,  1996,\n",
       "        2110,  1997,  5900,  1024,  2930,  1011,  2756,  1011, 20311,\n",
       "        1010,  5900,  3642,  1997,  2003, 13266,  2004,  4076,  1024,\n",
       "        3272,  2004,  4728,  3024,  1999,  1998,  2019,  3954,  1997,\n",
       "        3200,  1010,  2060,  2084,  1037,  4758,  9415,  1010,  6315,\n",
       "        3430,  2030, 11498, 27921, 12032,  2401,  1010,  2008,  2038,\n",
       "        2042,  8243,  4618,  5371,  2019,  3437,  2306,  4228,  2420,\n",
       "        2044,  1996,  6503,  1997,  2326,  1997,  2832,  1012,  2065,\n",
       "        2019,   102], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "determined-issue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[  101,  5900,  6372,  3180,  5219,  2000,  1024, 14814,  1037,\n",
       "         2011,  1024,  4387,  6243,  2160,  3021,  2019,  2552,  2000,\n",
       "        27950,  2930,  5900,  3642,  1997,  2000,  3166,  4697,  1996,\n",
       "         5900,  4879,  1997, 27290,  2000, 27764,  1997,  4895, 25154,\n",
       "         2098,  2512, 29278, 21156,  2098,  3200,  1999,  1996,  2168,\n",
       "         5450,  2004, 15726,  3200,  1025,  1998,  2005,  3141,  5682,\n",
       "         1012,  2022,  2009, 11955,  2011,  1996,  6372,  1997,  1996,\n",
       "         2110,  1997,  5900,  1024,  2930,  1011,  2756,  1011, 20311,\n",
       "         1010,  5900,  3642,  1997,  2003, 13266,  2004,  4076,  1024,\n",
       "         3272,  2004,  4728,  3024,  1999,  1998,  2019,  3954,  1997,\n",
       "         3200,  1010,  2060,  2084,  1037,  4758,  9415,  1010,  6315,\n",
       "         3430,  2030, 11498, 27921, 12032,  2401,  1010,  2008,  2038,\n",
       "         2042,  8243,  4618,  5371,  2019,  3437,  2306,  4228,  2420,\n",
       "         2044,  1996,  6503,  1997,  2326,  1997,  2832,  1012,  2065,\n",
       "         2019,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "      dtype=int32)>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-discipline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
