{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import LongformerTokenizer\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.data.read_parallel import read_parallel_local\n",
    "from src.models.deeplegis import *\n",
    "from src.models.data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of examples: 199646\n",
      "Reduced number of examples:  199646\n",
      "Took 0.4950267195701599 min to open 199646 files with 20 processes.\n"
     ]
    }
   ],
   "source": [
    "REDUCE_BY_FACTOR = 1 # Make the dataset smaller for development purposes\n",
    "train_test_ratio = 0.91\n",
    "train_valid_ratio = 0.90\n",
    "\n",
    "if 'DATA_VOL' not in os.environ:\n",
    "    # Manually set:\n",
    "    DATA_VOL = '/datavol/'\n",
    "else:\n",
    "    DATA_VOL = os.environ['DATA_VOL']\n",
    "    \n",
    "# Pre-wrangled metadata\n",
    "df = pd.read_csv(\"../references/derived/ml_data.csv\", encoding=\"latin1\", parse_dates=True)\n",
    "df.id = df.id.astype(int)    \n",
    "print(f\"Original number of examples: {len(df)}\")\n",
    "df = df.sample(n=int(len(df)/REDUCE_BY_FACTOR)) #\n",
    "print(f\"Reduced number of examples:  {len(df)}\")\n",
    "\n",
    "tmp = read_parallel_local(df['id'], DATA_VOL + \"/clean/\")\n",
    "df['text'] = tmp\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "sc_id_encoder = LabelEncoder()\n",
    "df['sc_id_cat'] = sc_id_encoder.fit_transform(df['sc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: (163509, 8)\n",
      "Validation size: (18168, 8)\n",
      "Test size: (17969, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFLongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "longformer (TFLongformerMainLay TFLongformerBaseMode 148068864   input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "partisan_lean (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "version_number (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sc_id (InputLayer)              [(None, 134)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (Sli (None, 768)          0           longformer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_9 (TFOpLambda)        (None, 136)          0           partisan_lean[0][0]              \n",
      "                                                                 version_number[0][0]             \n",
      "                                                                 sc_id[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_258 (Dropout)           (None, 768)          0           tf.__operators__.getitem_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "no_text_dense_layer (Dense)     (None, 700)          95900       tf.concat_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_10 (TFOpLambda)       (None, 1468)         0           dropout_258[0][0]                \n",
      "                                                                 no_text_dense_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "join_dense_layer (Dense)        (None, 700)          1028300     tf.concat_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_259 (Dropout)           (None, 700)          0           join_dense_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            701         dropout_259[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 149,193,765\n",
      "Trainable params: 149,193,765\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... get_config() missing 1 required positional argument: 'self'\n",
      "Epoch 1/10\n",
      "40877/40877 [==============================] - 29833s 727ms/step - loss: 0.2718 - binary_accuracy: 0.8896 - precision_4: 0.6597 - recall_4: 0.4001 - auc_4: 0.8720 - val_loss: 0.2568 - val_binary_accuracy: 0.8952 - val_precision_4: 0.7151 - val_recall_4: 0.4018 - val_auc_4: 0.8872\n",
      "\n",
      "Epoch 00001: saving model to /datavol/models/all_data/all_data.ckpt\n",
      "Epoch 2/10\n",
      " 5903/40877 [===>..........................] - ETA: 6:59:29 - loss: 0.2579 - binary_accuracy: 0.8955 - precision_4: 0.6910 - recall_4: 0.4246 - auc_4: 0.8857"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config['max_length'] = 128\n",
    "config['train_batch_size'] = 4\n",
    "config['testing'] = False\n",
    "config['train_test_ratio'] = 0.91\n",
    "config['train_valid_ratio'] = 0.90 \n",
    "config['tokenizer'] = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "config['n_sc_id_classes'] = len(sc_id_encoder.classes_)\n",
    "config['checkpoint_path'] = DATA_VOL + \"models/all_data/all_data.ckpt\"\n",
    "config['log_dir'] = DATA_VOL + \"logs/fit/all_data_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "config['epochs'] = 10\n",
    "config['learning_rate'] = 1e-4\n",
    "dl_all = deepLegisAll(config)\n",
    "dl_all.load_data(df)\n",
    "dl_all.build()\n",
    "dl_all.deep_legis_model.summary()\n",
    "dl_all.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = DATA_VOL + \"models/all_data/full_model.h5\"\n",
    "dl_all.deep_legis_model.save(model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3_8",
   "language": "python",
   "name": "3_8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
